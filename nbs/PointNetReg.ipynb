{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "display_name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "48fd179440322e061cbc970dcdf365600b1385398598600899ab36021fced37c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import scipy.spatial.distance\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import open3d as o3d\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "source": [
    "## TODO\n",
    "\n",
    "- NumpyのデータファイルからPytorchテンソルへの変換\n",
    "\n",
    "- Modelに投入できるようにする\n",
    "\n",
    "- 正規化の前処理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data.shape: (4, 10, 3)\n"
     ]
    }
   ],
   "source": [
    "# dummy Data\n",
    "\n",
    "data = np.random.randn(4, 10, 3)\n",
    "print(f\"data.shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_sizeになるように1データの点群数を削除\n",
    "class PointSampler(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, int)\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        return data[:self.output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.97869085 -0.04427212  2.15237277]\n [ 0.49597524  1.17119548  1.5125282 ]\n [-1.62430108 -1.77269708 -0.53545669]\n [-0.75024138  0.26540019  1.42908106]\n [ 1.38188116  0.9384979   0.29304972]]\n"
     ]
    }
   ],
   "source": [
    "pointcloud = PointSampler(5)(data[0])\n",
    "print(pointcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape) == 2\n",
    "\n",
    "        # x, y, z軸で平均を引く→各ベクトルの大きさの最大値で各要素を割る\n",
    "        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0)\n",
    "        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
    "\n",
    "        return norm_pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.31506618 -0.07202067  0.6449331 ]\n [ 0.22120289  0.36998968  0.41225069]\n [-0.54984529 -0.70057031 -0.33250838]\n [-0.23198948  0.04059308  0.38190476]\n [ 0.54336663  0.28536814 -0.03121824]\n [-0.05316551  0.56217761 -0.82530586]\n [ 0.14513996 -0.15547889  0.14297166]\n [-0.0348982  -0.14216677 -0.00644547]\n [-0.19131902 -0.16338044 -0.27635758]\n [ 0.46657419 -0.02451144 -0.11022467]]\n"
     ]
    }
   ],
   "source": [
    "# Note\n",
    "\n",
    "norm_pointcloud = Normalize()(data[0])\n",
    "print(norm_pointcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape) == 2\n",
    "\n",
    "        return torch.from_numpy(pointcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.3151, -0.0720,  0.6449],\n",
       "        [ 0.2212,  0.3700,  0.4123],\n",
       "        [-0.5498, -0.7006, -0.3325],\n",
       "        [-0.2320,  0.0406,  0.3819],\n",
       "        [ 0.5434,  0.2854, -0.0312],\n",
       "        [-0.0532,  0.5622, -0.8253],\n",
       "        [ 0.1451, -0.1555,  0.1430],\n",
       "        [-0.0349, -0.1422, -0.0064],\n",
       "        [-0.1913, -0.1634, -0.2764],\n",
       "        [ 0.4666, -0.0245, -0.1102]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "ToTensor()(norm_pointcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_transforms():\n",
    "    return transforms.Compose([\n",
    "        PointSampler(600),\n",
    "        Normalize(),\n",
    "        ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pcd(path):\n",
    "    pcd = o3d.open3d.io.read_point_cloud(path)\n",
    "    points = pcd.points\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Pytorch Datasetオブジェクトの作成\n",
    "\n",
    "class PointCloudData(Dataset):\n",
    "    def __init__(self, root_dir, valid=False, folder=\"train\", transform=default_transforms()):\n",
    "        self.root_dir = root_dir\n",
    "        folders = [dir for dir in sorted(os.listdir(\"Data/five_position_class\")) if os.path.isdir(f\"Data/five_position_class/{dir}\")]\n",
    "        self.classes = {folder:i for i, folder in enumerate(folders)}\n",
    "        self.transforms = transform if not valid else default_transforms()\n",
    "        self.valid = valid\n",
    "        self.files = []\n",
    "\n",
    "        for category in self.classes.keys():\n",
    "            new_dir = f\"Data/five_position_class/{category}/\"\n",
    "            for file in os.listdir(new_dir):\n",
    "                if file.endswith('.pcd'):\n",
    "                    # PCDファイルからpcd.points読み込み\n",
    "                    sample = {}\n",
    "                    sample['pcd_path'] = f\"{new_dir}/{file}\"\n",
    "                    sample['category'] = category\n",
    "                    self.files.append(sample)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __preproc__(self, path):\n",
    "        points = read_pcd(path)\n",
    "        if self.transforms:\n",
    "            pointcloud = self.transforms(points)\n",
    "        return pointcloud\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pcd_path = self.files[idx]['pcd_path']\n",
    "        category = self.files[idx]['category']\n",
    "        pointcloud = self.__preproc__(pcd_path)\n",
    "        return {'pointcloud': pointcloud, 'category': self.classes[category]}\n",
    "                    "
   ]
  },
  {
   "source": [
    "## Model\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Tnet(nn.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.conv1 = nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k*k)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input shape == (bs, n, 3)\n",
    "        bs = input.size(0)\n",
    "        xb = F.relu(self.bn1(self.conv1(input)))\n",
    "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
    "        xb = F.relu(self.bn3(self.conv3(xb)))\n",
    "        # maxPool/Flattenオブジェクトの作成→実行\n",
    "        pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
    "        flat = nn.Flatten(1)(pool)\n",
    "        xb = F.relu(self.bn4(self.fc1(flat)))\n",
    "        xb = F.relu(self.bn5(self.fc2(xb)))\n",
    "\n",
    "        # 対角行列の生成→それを複製\n",
    "        init = torch.eye(self.k, requires_grad=True).repeat(bs, 1, 1)\n",
    "        if xb.is_cuda:\n",
    "            init = init.cuda\n",
    "        # tensorサイズを自動的に調整して(view)、initを足している\n",
    "        matrix = self.fc3(xb).view(-1, self.k, self.k) + init\n",
    "        return matrix\n",
    "\n",
    "class Transform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_transform = Tnet(k=3)\n",
    "        self.feature_transform = Tnet(k=64)\n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 1024, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "\n",
    "    def forward(self, input):\n",
    "        matrix3x3 = self.input_transform(input)\n",
    "        # バッチごとに2次元×2次元の行列積を演算するので、3次元×3次元の計算をします\n",
    "        xb = torch.bmm(torch.transpose(input, 1, 2), matrix3x3).transpose(1, 2)\n",
    "\n",
    "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
    "\n",
    "        matrix64x64 = self.feature_transform(xb)\n",
    "        xb = torch.bmm(torch.transpose(xb, 1, 2), matrix64x64).transpose(1, 2)\n",
    "\n",
    "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
    "        xb = self.bn3(self.conv3(xb))\n",
    "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
    "        output = nn.Flatten(1)(xb)\n",
    "        return output, matrix3x3, matrix64x64\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transform = Transform()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 3)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.identity = nn.Identity()\n",
    "\n",
    "    def forward(self, input):\n",
    "        xb, matrix3x3, matrix64x64 = self.transform(input)\n",
    "        xb = F.relu(self.bn1(self.fc1(xb)))\n",
    "        xb = F.relu(self.bn2(self.fc2(xb)))\n",
    "        output = self.fc3(xb)\n",
    "        return self.identity(output), matrix3x3, matrix64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[1. 2. 3.]\n  [4. 5. 6.]\n  [7. 8. 9.]]\n\n [[2. 3. 4.]\n  [2. 3. 5.]\n  [2. 1. 6.]]]\ntensor([[[1., 4., 7.],\n         [2., 5., 8.],\n         [3., 6., 9.]],\n\n        [[2., 2., 2.],\n         [3., 3., 1.],\n         [4., 5., 6.]]])\n"
     ]
    }
   ],
   "source": [
    "## Notes\n",
    "\n",
    "nm = np.array([[[1.0, 2.0, 3.0],\n",
    "                [4.0, 5.0, 6.0],\n",
    "                [7.0, 8.0, 9.0]],\n",
    "                \n",
    "                [[2.0, 3.0, 4.0],\n",
    "                #  [1.0, 2.0, 3.0],\n",
    "                 [2.0, 3.0, 5.0],\n",
    "                 [2.0, 1.0, 6.0]]])\n",
    "# nm = np.random.randn(2, 10, 3)\n",
    "print(nm)\n",
    "nm = nm.astype(np.float32)\n",
    "x = torch.from_numpy(nm)\n",
    "x = x.transpose(1, 2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.1998,  0.3639,  0.4269],\n        [ 0.4509,  0.5154, -0.2120]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Notes\n",
    "\n",
    "tnet = Tnet()\n",
    "y = tnet(x)\n",
    "# print(y)\n",
    "# print(y[0])\n",
    "\n",
    "pn = PointNet()\n",
    "y, m3, m64 = pn(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointnetloss(outputs, m3x3, m64x64, alpha=0.0001):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    bs = outputs.size(0)\n",
    "    id3x3 = torch.eye(3, requires_grad=True).repeat(bs, 1, 1)\n",
    "    id64x64 = torch.eye(64, requires_grad=True).repeat(bs, 1, 1)\n",
    "    if outputs.is_cuda:\n",
    "        id3x3 = id3x3.cuda()\n",
    "        id64x64 = id64x64.cuda()\n",
    "    diff3x3 = id3x3 - torch.bmm(m3x3, m3x3.transpose(1, 2))\n",
    "    diff64x64 = id64x64 - torch.bmm(m64x64, m64x64.transpose(1, 2))\n",
    "    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3) + torch.norm(diff64x64)) / float(bs)"
   ]
  },
  {
   "source": [
    "## TrainingLoop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointnet = PointNet()\n",
    "pointnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader=None, epochs=15, save=True):\n",
    "    for epoch in range(epochs):\n",
    "        pointnet.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs, m3x3, m64x64 = pointnet(inputs.transpose(1, 2))\n",
    "\n",
    "            loss = pointnetloss(outputs, labels, m3x3, m64x64)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statics\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9: # print every 10 mini-batches\n",
    "                print('[Epoch: %d, Batch: %4d / %4d], loss: %.3f' %\n",
    "                    (epoch+1, i+1, len(train_loader), running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        pointnet.eval()\n",
    "        correct = total = 0\n",
    "\n",
    "        # validation\n",
    "        if val_loader:\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device).float()\n",
    "                    predicted, __, __ = pointnet(inputs.transpose(1, 2))\n",
    "                    mse_loss = torch.nn.MSELoss()\n",
    "                    print('Mean Squared Error: %.3f' % mse_loss(predicted, labels))\n",
    "        \n",
    "        if save:\n",
    "            torch.save(pointnet.state_dict(), \"save_\" + str(epochs) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(pointnet, train_loader, valid_loader, save=True)"
   ]
  }
 ]
}