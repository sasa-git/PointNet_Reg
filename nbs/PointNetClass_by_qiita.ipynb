{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "display_name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "48fd179440322e061cbc970dcdf365600b1385398598600899ab36021fced37c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "https://qiita.com/opeco17/items/707a5c57bca41a145122"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import scipy.spatial.distance\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLinear(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(NonLinear, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.input_channels, self.output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(self.output_channels))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool(nn.Module):\n",
    "    def __init__(self, num_channels, num_points):\n",
    "        super(MaxPool, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_points = num_points\n",
    "        self.main = nn.MaxPool1d(self.num_points)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = input.view(-1, self.num_channels, self.num_points)\n",
    "        out = self.main(out)\n",
    "        out = out.view(-1, self.num_channels)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputTNet(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            NonLinear(3, 64),\n",
    "            NonLinear(64, 128),\n",
    "            NonLinear(128, 1024),\n",
    "            MaxPool(1024, self.num_points),\n",
    "            NonLinear(1024, 512),\n",
    "            NonLinear(512, 256),\n",
    "            nn.Linear(256, 9)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        matrix = self.main(input).view(-1, 3, 3)\n",
    "        out = torch.matmul(input.view(-1, self.num_points, 3), matrix)\n",
    "        out = out.view(-1, 3)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTNet(nn.Module):\n",
    "    def __init__(self, num_points):\n",
    "        super(FeatureTNet, self).__init__()\n",
    "        self.num_points = num_points\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            NonLinear(64, 64),\n",
    "            NonLinear(64, 128),\n",
    "            NonLinear(128, 1024),\n",
    "            MaxPool(1024, self.num_points),\n",
    "            NonLinear(1024, 512),\n",
    "            NonLinear(512, 256),\n",
    "            nn.Linear(256, 4096)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        matrix = self.main(input).view(-1, 64, 64)\n",
    "        out = torch.matmul(input.view(-1, self.num_points, 64), matrix)\n",
    "        out = out.view(-1, 64)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet(nn.Module):\n",
    "    def __init__(self, num_points, num_labels):\n",
    "        super(PointNet, self).__init__()\n",
    "        self.num_points = num_points\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            InputTNet(self.num_points),\n",
    "            NonLinear(3, 64),\n",
    "            NonLinear(64, 64),\n",
    "            FeatureTNet(self.num_points),\n",
    "            NonLinear(64, 64),\n",
    "            NonLinear(64, 128),\n",
    "            NonLinear(128, 1024),\n",
    "            MaxPool(1024, self.num_points),\n",
    "            NonLinear(1024, 512),\n",
    "            nn.Dropout(p=0.3),\n",
    "            NonLinear(512, 256),\n",
    "            nn.Dropout(p=0.3),\n",
    "            NonLinear(256, self.num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sampler(batch_size, num_points):\n",
    "    half_batch_size = int(batch_size/2)\n",
    "    normal_sampled = torch.randn(half_batch_size, num_points, 3)\n",
    "    uniform_sampled = torch.rand(half_batch_size, num_points, 3)\n",
    "    normal_labels = torch.ones(half_batch_size)\n",
    "    uniform_labels = torch.zeros(half_batch_size)\n",
    "\n",
    "    input_data = torch.cat((normal_sampled, uniform_sampled), dim=0)\n",
    "    labels = torch.cat((normal_labels, uniform_labels), dim=0)\n",
    "\n",
    "    data_shuffle = torch.randperm(batch_size)\n",
    "\n",
    "    return input_data[data_shuffle].view(-1, 3), labels[data_shuffle].view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ration: 7490    Loss: 0.0060462504625320435\n",
      "Iteration: 7490    Accuracy: 1.0\n",
      "Iteration: 7500    Loss: 0.006015968043357134\n",
      "Iteration: 7500    Accuracy: 1.0\n",
      "Iteration: 7510    Loss: 0.005978365894407034\n",
      "Iteration: 7510    Accuracy: 1.0\n",
      "Iteration: 7520    Loss: 0.005948164034634829\n",
      "Iteration: 7520    Accuracy: 1.0\n",
      "Iteration: 7530    Loss: 0.00598184997215867\n",
      "Iteration: 7530    Accuracy: 1.0\n",
      "Iteration: 7540    Loss: 0.005906805396080017\n",
      "Iteration: 7540    Accuracy: 1.0\n",
      "Iteration: 7550    Loss: 0.00591204734519124\n",
      "Iteration: 7550    Accuracy: 1.0\n",
      "Iteration: 7560    Loss: 0.00590001605451107\n",
      "Iteration: 7560    Accuracy: 1.0\n",
      "Iteration: 7570    Loss: 0.005846172571182251\n",
      "Iteration: 7570    Accuracy: 1.0\n",
      "Iteration: 7580    Loss: 0.0058439914137125015\n",
      "Iteration: 7580    Accuracy: 1.0\n",
      "Iteration: 7590    Loss: 0.005805515684187412\n",
      "Iteration: 7590    Accuracy: 1.0\n",
      "Iteration: 7600    Loss: 0.0057623437605798244\n",
      "Iteration: 7600    Accuracy: 1.0\n",
      "Iteration: 7610    Loss: 0.005750629585236311\n",
      "Iteration: 7610    Accuracy: 1.0\n",
      "Iteration: 7620    Loss: 0.005776088684797287\n",
      "Iteration: 7620    Accuracy: 1.0\n",
      "Iteration: 7630    Loss: 0.005711724050343037\n",
      "Iteration: 7630    Accuracy: 1.0\n",
      "Iteration: 7640    Loss: 0.005654801614582539\n",
      "Iteration: 7640    Accuracy: 1.0\n",
      "Iteration: 7650    Loss: 0.005630393046885729\n",
      "Iteration: 7650    Accuracy: 1.0\n",
      "Iteration: 7660    Loss: 0.00561124412342906\n",
      "Iteration: 7660    Accuracy: 1.0\n",
      "Iteration: 7670    Loss: 0.005616611335426569\n",
      "Iteration: 7670    Accuracy: 1.0\n",
      "Iteration: 7680    Loss: 0.005587893072515726\n",
      "Iteration: 7680    Accuracy: 1.0\n",
      "Iteration: 7690    Loss: 0.005546480882912874\n",
      "Iteration: 7690    Accuracy: 1.0\n",
      "Iteration: 7700    Loss: 0.005550991743803024\n",
      "Iteration: 7700    Accuracy: 1.0\n",
      "Iteration: 7710    Loss: 0.005476803053170443\n",
      "Iteration: 7710    Accuracy: 1.0\n",
      "Iteration: 7720    Loss: 0.00549386627972126\n",
      "Iteration: 7720    Accuracy: 1.0\n",
      "Iteration: 7730    Loss: 0.0054456256330013275\n",
      "Iteration: 7730    Accuracy: 1.0\n",
      "Iteration: 7740    Loss: 0.005430563353002071\n",
      "Iteration: 7740    Accuracy: 1.0\n",
      "Iteration: 7750    Loss: 0.005388327874243259\n",
      "Iteration: 7750    Accuracy: 1.0\n",
      "Iteration: 7760    Loss: 0.005373982712626457\n",
      "Iteration: 7760    Accuracy: 1.0\n",
      "Iteration: 7770    Loss: 0.005405038129538298\n",
      "Iteration: 7770    Accuracy: 1.0\n",
      "Iteration: 7780    Loss: 0.0053706043399870396\n",
      "Iteration: 7780    Accuracy: 1.0\n",
      "Iteration: 7790    Loss: 0.005331454798579216\n",
      "Iteration: 7790    Accuracy: 1.0\n",
      "Iteration: 7800    Loss: 0.005289417691528797\n",
      "Iteration: 7800    Accuracy: 1.0\n",
      "Iteration: 7810    Loss: 0.005262467078864574\n",
      "Iteration: 7810    Accuracy: 1.0\n",
      "Iteration: 7820    Loss: 0.005236102268099785\n",
      "Iteration: 7820    Accuracy: 1.0\n",
      "Iteration: 7830    Loss: 0.005215832032263279\n",
      "Iteration: 7830    Accuracy: 1.0\n",
      "Iteration: 7840    Loss: 0.005190408322960138\n",
      "Iteration: 7840    Accuracy: 1.0\n",
      "Iteration: 7850    Loss: 0.005191607400774956\n",
      "Iteration: 7850    Accuracy: 1.0\n",
      "Iteration: 7860    Loss: 0.00513006467372179\n",
      "Iteration: 7860    Accuracy: 1.0\n",
      "Iteration: 7870    Loss: 0.0051218848675489426\n",
      "Iteration: 7870    Accuracy: 1.0\n",
      "Iteration: 7880    Loss: 0.005109064280986786\n",
      "Iteration: 7880    Accuracy: 1.0\n",
      "Iteration: 7890    Loss: 0.0051220436580479145\n",
      "Iteration: 7890    Accuracy: 1.0\n",
      "Iteration: 7900    Loss: 0.005066419951617718\n",
      "Iteration: 7900    Accuracy: 1.0\n",
      "Iteration: 7910    Loss: 0.005069528706371784\n",
      "Iteration: 7910    Accuracy: 1.0\n",
      "Iteration: 7920    Loss: 0.00506823044270277\n",
      "Iteration: 7920    Accuracy: 1.0\n",
      "Iteration: 7930    Loss: 0.005001728422939777\n",
      "Iteration: 7930    Accuracy: 1.0\n",
      "Iteration: 7940    Loss: 0.004971615504473448\n",
      "Iteration: 7940    Accuracy: 1.0\n",
      "Iteration: 7950    Loss: 0.004960514139384031\n",
      "Iteration: 7950    Accuracy: 1.0\n",
      "Iteration: 7960    Loss: 0.005001291632652283\n",
      "Iteration: 7960    Accuracy: 1.0\n",
      "Iteration: 7970    Loss: 0.004948775749653578\n",
      "Iteration: 7970    Accuracy: 1.0\n",
      "Iteration: 7980    Loss: 0.004912429954856634\n",
      "Iteration: 7980    Accuracy: 1.0\n",
      "Iteration: 7990    Loss: 0.004859976004809141\n",
      "Iteration: 7990    Accuracy: 1.0\n",
      "Iteration: 8000    Loss: 0.004837389569729567\n",
      "Iteration: 8000    Accuracy: 1.0\n",
      "Iteration: 8010    Loss: 0.004855538718402386\n",
      "Iteration: 8010    Accuracy: 1.0\n",
      "Iteration: 8020    Loss: 0.004843066446483135\n",
      "Iteration: 8020    Accuracy: 1.0\n",
      "Iteration: 8030    Loss: 0.0047978851944208145\n",
      "Iteration: 8030    Accuracy: 1.0\n",
      "Iteration: 8040    Loss: 0.004779510665684938\n",
      "Iteration: 8040    Accuracy: 1.0\n",
      "Iteration: 8050    Loss: 0.004766296595335007\n",
      "Iteration: 8050    Accuracy: 1.0\n",
      "Iteration: 8060    Loss: 0.004728738218545914\n",
      "Iteration: 8060    Accuracy: 1.0\n",
      "Iteration: 8070    Loss: 0.004724352154880762\n",
      "Iteration: 8070    Accuracy: 1.0\n",
      "Iteration: 8080    Loss: 0.0047273957170546055\n",
      "Iteration: 8080    Accuracy: 1.0\n",
      "Iteration: 8090    Loss: 0.0046975016593933105\n",
      "Iteration: 8090    Accuracy: 1.0\n",
      "Iteration: 8100    Loss: 0.004653473850339651\n",
      "Iteration: 8100    Accuracy: 1.0\n",
      "Iteration: 8110    Loss: 0.004637293983250856\n",
      "Iteration: 8110    Accuracy: 1.0\n",
      "Iteration: 8120    Loss: 0.004613589029759169\n",
      "Iteration: 8120    Accuracy: 1.0\n",
      "Iteration: 8130    Loss: 0.004620349034667015\n",
      "Iteration: 8130    Accuracy: 1.0\n",
      "Iteration: 8140    Loss: 0.004583337809890509\n",
      "Iteration: 8140    Accuracy: 1.0\n",
      "Iteration: 8150    Loss: 0.0045920442789793015\n",
      "Iteration: 8150    Accuracy: 1.0\n",
      "Iteration: 8160    Loss: 0.004556284286081791\n",
      "Iteration: 8160    Accuracy: 1.0\n",
      "Iteration: 8170    Loss: 0.004516765475273132\n",
      "Iteration: 8170    Accuracy: 1.0\n",
      "Iteration: 8180    Loss: 0.0044962032698094845\n",
      "Iteration: 8180    Accuracy: 1.0\n",
      "Iteration: 8190    Loss: 0.004498173948377371\n",
      "Iteration: 8190    Accuracy: 1.0\n",
      "Iteration: 8200    Loss: 0.004462248180061579\n",
      "Iteration: 8200    Accuracy: 1.0\n",
      "Iteration: 8210    Loss: 0.004454361740499735\n",
      "Iteration: 8210    Accuracy: 1.0\n",
      "Iteration: 8220    Loss: 0.004426832776516676\n",
      "Iteration: 8220    Accuracy: 1.0\n",
      "Iteration: 8230    Loss: 0.0044142757542431355\n",
      "Iteration: 8230    Accuracy: 1.0\n",
      "Iteration: 8240    Loss: 0.004395134747028351\n",
      "Iteration: 8240    Accuracy: 1.0\n",
      "Iteration: 8250    Loss: 0.004376350902020931\n",
      "Iteration: 8250    Accuracy: 1.0\n",
      "Iteration: 8260    Loss: 0.004358774516731501\n",
      "Iteration: 8260    Accuracy: 1.0\n",
      "Iteration: 8270    Loss: 0.004341571591794491\n",
      "Iteration: 8270    Accuracy: 1.0\n",
      "Iteration: 8280    Loss: 0.004322654101997614\n",
      "Iteration: 8280    Accuracy: 1.0\n",
      "Iteration: 8290    Loss: 0.004293402656912804\n",
      "Iteration: 8290    Accuracy: 1.0\n",
      "Iteration: 8300    Loss: 0.00428545568138361\n",
      "Iteration: 8300    Accuracy: 1.0\n",
      "Iteration: 8310    Loss: 0.0042472523637115955\n",
      "Iteration: 8310    Accuracy: 1.0\n",
      "Iteration: 8320    Loss: 0.004233993589878082\n",
      "Iteration: 8320    Accuracy: 1.0\n",
      "Iteration: 8330    Loss: 0.004237097688019276\n",
      "Iteration: 8330    Accuracy: 1.0\n",
      "Iteration: 8340    Loss: 0.004224584437906742\n",
      "Iteration: 8340    Accuracy: 1.0\n",
      "Iteration: 8350    Loss: 0.004187275655567646\n",
      "Iteration: 8350    Accuracy: 1.0\n",
      "Iteration: 8360    Loss: 0.00416243402287364\n",
      "Iteration: 8360    Accuracy: 1.0\n",
      "Iteration: 8370    Loss: 0.004140951670706272\n",
      "Iteration: 8370    Accuracy: 1.0\n",
      "Iteration: 8380    Loss: 0.004145623184740543\n",
      "Iteration: 8380    Accuracy: 1.0\n",
      "Iteration: 8390    Loss: 0.004128941334784031\n",
      "Iteration: 8390    Accuracy: 1.0\n",
      "Iteration: 8400    Loss: 0.004105614963918924\n",
      "Iteration: 8400    Accuracy: 1.0\n",
      "Iteration: 8410    Loss: 0.004085618071258068\n",
      "Iteration: 8410    Accuracy: 1.0\n",
      "Iteration: 8420    Loss: 0.004058327525854111\n",
      "Iteration: 8420    Accuracy: 1.0\n",
      "Iteration: 8430    Loss: 0.004046824295073748\n",
      "Iteration: 8430    Accuracy: 1.0\n",
      "Iteration: 8440    Loss: 0.004035158082842827\n",
      "Iteration: 8440    Accuracy: 1.0\n",
      "Iteration: 8450    Loss: 0.0040158722549676895\n",
      "Iteration: 8450    Accuracy: 1.0\n",
      "Iteration: 8460    Loss: 0.004004685673862696\n",
      "Iteration: 8460    Accuracy: 1.0\n",
      "Iteration: 8470    Loss: 0.003968778066337109\n",
      "Iteration: 8470    Accuracy: 1.0\n",
      "Iteration: 8480    Loss: 0.003971986006945372\n",
      "Iteration: 8480    Accuracy: 1.0\n",
      "Iteration: 8490    Loss: 0.003947427961975336\n",
      "Iteration: 8490    Accuracy: 1.0\n",
      "Iteration: 8500    Loss: 0.003942970186471939\n",
      "Iteration: 8500    Accuracy: 1.0\n",
      "Iteration: 8510    Loss: 0.0039322772063314915\n",
      "Iteration: 8510    Accuracy: 1.0\n",
      "Iteration: 8520    Loss: 0.003909459803253412\n",
      "Iteration: 8520    Accuracy: 1.0\n",
      "Iteration: 8530    Loss: 0.0038976953364908695\n",
      "Iteration: 8530    Accuracy: 1.0\n",
      "Iteration: 8540    Loss: 0.003864369820803404\n",
      "Iteration: 8540    Accuracy: 1.0\n",
      "Iteration: 8550    Loss: 0.0038788216188549995\n",
      "Iteration: 8550    Accuracy: 1.0\n",
      "Iteration: 8560    Loss: 0.0038421181961894035\n",
      "Iteration: 8560    Accuracy: 1.0\n",
      "Iteration: 8570    Loss: 0.003836799645796418\n",
      "Iteration: 8570    Accuracy: 1.0\n",
      "Iteration: 8580    Loss: 0.003796020057052374\n",
      "Iteration: 8580    Accuracy: 1.0\n",
      "Iteration: 8590    Loss: 0.0037922016344964504\n",
      "Iteration: 8590    Accuracy: 1.0\n",
      "Iteration: 8600    Loss: 0.0037679332308471203\n",
      "Iteration: 8600    Accuracy: 1.0\n",
      "Iteration: 8610    Loss: 0.003749527269974351\n",
      "Iteration: 8610    Accuracy: 1.0\n",
      "Iteration: 8620    Loss: 0.003727560630068183\n",
      "Iteration: 8620    Accuracy: 1.0\n",
      "Iteration: 8630    Loss: 0.0037255531642585993\n",
      "Iteration: 8630    Accuracy: 1.0\n",
      "Iteration: 8640    Loss: 0.0037227612920105457\n",
      "Iteration: 8640    Accuracy: 1.0\n",
      "Iteration: 8650    Loss: 0.0037140408530831337\n",
      "Iteration: 8650    Accuracy: 1.0\n",
      "Iteration: 8660    Loss: 0.0036741544026881456\n",
      "Iteration: 8660    Accuracy: 1.0\n",
      "Iteration: 8670    Loss: 0.0036477658431977034\n",
      "Iteration: 8670    Accuracy: 1.0\n",
      "Iteration: 8680    Loss: 0.0036319978535175323\n",
      "Iteration: 8680    Accuracy: 1.0\n",
      "Iteration: 8690    Loss: 0.003620775882154703\n",
      "Iteration: 8690    Accuracy: 1.0\n",
      "Iteration: 8700    Loss: 0.0036173288244754076\n",
      "Iteration: 8700    Accuracy: 1.0\n",
      "Iteration: 8710    Loss: 0.003596864640712738\n",
      "Iteration: 8710    Accuracy: 1.0\n",
      "Iteration: 8720    Loss: 0.0035871106665581465\n",
      "Iteration: 8720    Accuracy: 1.0\n",
      "Iteration: 8730    Loss: 0.0035712600219994783\n",
      "Iteration: 8730    Accuracy: 1.0\n",
      "Iteration: 8740    Loss: 0.0035493620671331882\n",
      "Iteration: 8740    Accuracy: 1.0\n",
      "Iteration: 8750    Loss: 0.0035302566830068827\n",
      "Iteration: 8750    Accuracy: 1.0\n",
      "Iteration: 8760    Loss: 0.003509136149659753\n",
      "Iteration: 8760    Accuracy: 1.0\n",
      "Iteration: 8770    Loss: 0.0034983668010681868\n",
      "Iteration: 8770    Accuracy: 1.0\n",
      "Iteration: 8780    Loss: 0.003474828088656068\n",
      "Iteration: 8780    Accuracy: 1.0\n",
      "Iteration: 8790    Loss: 0.003483331063762307\n",
      "Iteration: 8790    Accuracy: 1.0\n",
      "Iteration: 8800    Loss: 0.0034634016919881105\n",
      "Iteration: 8800    Accuracy: 1.0\n",
      "Iteration: 8810    Loss: 0.003445293987169862\n",
      "Iteration: 8810    Accuracy: 1.0\n",
      "Iteration: 8820    Loss: 0.0034343027509748936\n",
      "Iteration: 8820    Accuracy: 1.0\n",
      "Iteration: 8830    Loss: 0.003407372860237956\n",
      "Iteration: 8830    Accuracy: 1.0\n",
      "Iteration: 8840    Loss: 0.003400071756914258\n",
      "Iteration: 8840    Accuracy: 1.0\n",
      "Iteration: 8850    Loss: 0.003378466237336397\n",
      "Iteration: 8850    Accuracy: 1.0\n",
      "Iteration: 8860    Loss: 0.003374521154910326\n",
      "Iteration: 8860    Accuracy: 1.0\n",
      "Iteration: 8870    Loss: 0.003356603905558586\n",
      "Iteration: 8870    Accuracy: 1.0\n",
      "Iteration: 8880    Loss: 0.0033433164935559034\n",
      "Iteration: 8880    Accuracy: 1.0\n",
      "Iteration: 8890    Loss: 0.0033546299673616886\n",
      "Iteration: 8890    Accuracy: 1.0\n",
      "Iteration: 8900    Loss: 0.003364008152857423\n",
      "Iteration: 8900    Accuracy: 1.0\n",
      "Iteration: 8910    Loss: 0.0033228974789381027\n",
      "Iteration: 8910    Accuracy: 1.0\n",
      "Iteration: 8920    Loss: 0.003297284245491028\n",
      "Iteration: 8920    Accuracy: 1.0\n",
      "Iteration: 8930    Loss: 0.0032718200236558914\n",
      "Iteration: 8930    Accuracy: 1.0\n",
      "Iteration: 8940    Loss: 0.003267130348831415\n",
      "Iteration: 8940    Accuracy: 1.0\n",
      "Iteration: 8950    Loss: 0.003251476678997278\n",
      "Iteration: 8950    Accuracy: 1.0\n",
      "Iteration: 8960    Loss: 0.003230169415473938\n",
      "Iteration: 8960    Accuracy: 1.0\n",
      "Iteration: 8970    Loss: 0.003213520860299468\n",
      "Iteration: 8970    Accuracy: 1.0\n",
      "Iteration: 8980    Loss: 0.003208484733477235\n",
      "Iteration: 8980    Accuracy: 1.0\n",
      "Iteration: 8990    Loss: 0.0031839620787650347\n",
      "Iteration: 8990    Accuracy: 1.0\n",
      "Iteration: 9000    Loss: 0.00318764615803957\n",
      "Iteration: 9000    Accuracy: 1.0\n",
      "Iteration: 9010    Loss: 0.003163805929943919\n",
      "Iteration: 9010    Accuracy: 1.0\n",
      "Iteration: 9020    Loss: 0.0031933276914060116\n",
      "Iteration: 9020    Accuracy: 1.0\n",
      "Iteration: 9030    Loss: 0.0031472896225750446\n",
      "Iteration: 9030    Accuracy: 1.0\n",
      "Iteration: 9040    Loss: 0.0031222233083099127\n",
      "Iteration: 9040    Accuracy: 1.0\n",
      "Iteration: 9050    Loss: 0.0031093761790543795\n",
      "Iteration: 9050    Accuracy: 1.0\n",
      "Iteration: 9060    Loss: 0.0030937744304537773\n",
      "Iteration: 9060    Accuracy: 1.0\n",
      "Iteration: 9070    Loss: 0.003080019261687994\n",
      "Iteration: 9070    Accuracy: 1.0\n",
      "Iteration: 9080    Loss: 0.0030655348673462868\n",
      "Iteration: 9080    Accuracy: 1.0\n",
      "Iteration: 9090    Loss: 0.0030574786942452192\n",
      "Iteration: 9090    Accuracy: 1.0\n",
      "Iteration: 9100    Loss: 0.0030468127224594355\n",
      "Iteration: 9100    Accuracy: 1.0\n",
      "Iteration: 9110    Loss: 0.003043273463845253\n",
      "Iteration: 9110    Accuracy: 1.0\n",
      "Iteration: 9120    Loss: 0.0030202274210751057\n",
      "Iteration: 9120    Accuracy: 1.0\n",
      "Iteration: 9130    Loss: 0.0030051912181079388\n",
      "Iteration: 9130    Accuracy: 1.0\n",
      "Iteration: 9140    Loss: 0.0029917352367192507\n",
      "Iteration: 9140    Accuracy: 1.0\n",
      "Iteration: 9150    Loss: 0.002974533708766103\n",
      "Iteration: 9150    Accuracy: 1.0\n",
      "Iteration: 9160    Loss: 0.0029641734436154366\n",
      "Iteration: 9160    Accuracy: 1.0\n",
      "Iteration: 9170    Loss: 0.002951775211840868\n",
      "Iteration: 9170    Accuracy: 1.0\n",
      "Iteration: 9180    Loss: 0.0029380021151155233\n",
      "Iteration: 9180    Accuracy: 1.0\n",
      "Iteration: 9190    Loss: 0.002935066819190979\n",
      "Iteration: 9190    Accuracy: 1.0\n",
      "Iteration: 9200    Loss: 0.0029181172139942646\n",
      "Iteration: 9200    Accuracy: 1.0\n",
      "Iteration: 9210    Loss: 0.0029069765005260706\n",
      "Iteration: 9210    Accuracy: 1.0\n",
      "Iteration: 9220    Loss: 0.002898886101320386\n",
      "Iteration: 9220    Accuracy: 1.0\n",
      "Iteration: 9230    Loss: 0.0028825895860791206\n",
      "Iteration: 9230    Accuracy: 1.0\n",
      "Iteration: 9240    Loss: 0.0028681564144790173\n",
      "Iteration: 9240    Accuracy: 1.0\n",
      "Iteration: 9250    Loss: 0.002856223611161113\n",
      "Iteration: 9250    Accuracy: 1.0\n",
      "Iteration: 9260    Loss: 0.0028434472624212503\n",
      "Iteration: 9260    Accuracy: 1.0\n",
      "Iteration: 9270    Loss: 0.0028309791814535856\n",
      "Iteration: 9270    Accuracy: 1.0\n",
      "Iteration: 9280    Loss: 0.0028191721066832542\n",
      "Iteration: 9280    Accuracy: 1.0\n",
      "Iteration: 9290    Loss: 0.0028063002973794937\n",
      "Iteration: 9290    Accuracy: 1.0\n",
      "Iteration: 9300    Loss: 0.0027930019423365593\n",
      "Iteration: 9300    Accuracy: 1.0\n",
      "Iteration: 9310    Loss: 0.002780439332127571\n",
      "Iteration: 9310    Accuracy: 1.0\n",
      "Iteration: 9320    Loss: 0.0027683200314641\n",
      "Iteration: 9320    Accuracy: 1.0\n",
      "Iteration: 9330    Loss: 0.002759120659902692\n",
      "Iteration: 9330    Accuracy: 1.0\n",
      "Iteration: 9340    Loss: 0.0027508579660207033\n",
      "Iteration: 9340    Accuracy: 1.0\n",
      "Iteration: 9350    Loss: 0.002737813163548708\n",
      "Iteration: 9350    Accuracy: 1.0\n",
      "Iteration: 9360    Loss: 0.0027238859329372644\n",
      "Iteration: 9360    Accuracy: 1.0\n",
      "Iteration: 9370    Loss: 0.0027140704914927483\n",
      "Iteration: 9370    Accuracy: 1.0\n",
      "Iteration: 9380    Loss: 0.00270404783077538\n",
      "Iteration: 9380    Accuracy: 1.0\n",
      "Iteration: 9390    Loss: 0.002690729219466448\n",
      "Iteration: 9390    Accuracy: 1.0\n",
      "Iteration: 9400    Loss: 0.0026796970050781965\n",
      "Iteration: 9400    Accuracy: 1.0\n",
      "Iteration: 9410    Loss: 0.002668467117473483\n",
      "Iteration: 9410    Accuracy: 1.0\n",
      "Iteration: 9420    Loss: 0.0026551801711320877\n",
      "Iteration: 9420    Accuracy: 1.0\n",
      "Iteration: 9430    Loss: 0.002645561471581459\n",
      "Iteration: 9430    Accuracy: 1.0\n",
      "Iteration: 9440    Loss: 0.0026357462629675865\n",
      "Iteration: 9440    Accuracy: 1.0\n",
      "Iteration: 9450    Loss: 0.0026262367609888315\n",
      "Iteration: 9450    Accuracy: 1.0\n",
      "Iteration: 9460    Loss: 0.0026129446923732758\n",
      "Iteration: 9460    Accuracy: 1.0\n",
      "Iteration: 9470    Loss: 0.002600752981379628\n",
      "Iteration: 9470    Accuracy: 1.0\n",
      "Iteration: 9480    Loss: 0.002589690964668989\n",
      "Iteration: 9480    Accuracy: 1.0\n",
      "Iteration: 9490    Loss: 0.002580096013844013\n",
      "Iteration: 9490    Accuracy: 1.0\n",
      "Iteration: 9500    Loss: 0.0025682460982352495\n",
      "Iteration: 9500    Accuracy: 1.0\n",
      "Iteration: 9510    Loss: 0.0025569801218807697\n",
      "Iteration: 9510    Accuracy: 1.0\n",
      "Iteration: 9520    Loss: 0.0025462820194661617\n",
      "Iteration: 9520    Accuracy: 1.0\n",
      "Iteration: 9530    Loss: 0.002535224659368396\n",
      "Iteration: 9530    Accuracy: 1.0\n",
      "Iteration: 9540    Loss: 0.002525458810850978\n",
      "Iteration: 9540    Accuracy: 1.0\n",
      "Iteration: 9550    Loss: 0.0025167111307382584\n",
      "Iteration: 9550    Accuracy: 1.0\n",
      "Iteration: 9560    Loss: 0.0025053806602954865\n",
      "Iteration: 9560    Accuracy: 1.0\n",
      "Iteration: 9570    Loss: 0.002494670217856765\n",
      "Iteration: 9570    Accuracy: 1.0\n",
      "Iteration: 9580    Loss: 0.0024839595425873995\n",
      "Iteration: 9580    Accuracy: 1.0\n",
      "Iteration: 9590    Loss: 0.0024735720362514257\n",
      "Iteration: 9590    Accuracy: 1.0\n",
      "Iteration: 9600    Loss: 0.00246298685669899\n",
      "Iteration: 9600    Accuracy: 1.0\n",
      "Iteration: 9610    Loss: 0.0024523520842194557\n",
      "Iteration: 9610    Accuracy: 1.0\n",
      "Iteration: 9620    Loss: 0.0024422318674623966\n",
      "Iteration: 9620    Accuracy: 1.0\n",
      "Iteration: 9630    Loss: 0.0024319556541740894\n",
      "Iteration: 9630    Accuracy: 1.0\n",
      "Iteration: 9640    Loss: 0.0024218277540057898\n",
      "Iteration: 9640    Accuracy: 1.0\n",
      "Iteration: 9650    Loss: 0.0024118577130138874\n",
      "Iteration: 9650    Accuracy: 1.0\n",
      "Iteration: 9660    Loss: 0.0024021370336413383\n",
      "Iteration: 9660    Accuracy: 1.0\n",
      "Iteration: 9670    Loss: 0.0023918605875223875\n",
      "Iteration: 9670    Accuracy: 1.0\n",
      "Iteration: 9680    Loss: 0.0023816870525479317\n",
      "Iteration: 9680    Accuracy: 1.0\n",
      "Iteration: 9690    Loss: 0.002371807349845767\n",
      "Iteration: 9690    Accuracy: 1.0\n",
      "Iteration: 9700    Loss: 0.0023622740991413593\n",
      "Iteration: 9700    Accuracy: 1.0\n",
      "Iteration: 9710    Loss: 0.0023524886928498745\n",
      "Iteration: 9710    Accuracy: 1.0\n",
      "Iteration: 9720    Loss: 0.0023735156282782555\n",
      "Iteration: 9720    Accuracy: 1.0\n",
      "Iteration: 9730    Loss: 0.0023687640205025673\n",
      "Iteration: 9730    Accuracy: 1.0\n",
      "Iteration: 9740    Loss: 0.002347490983083844\n",
      "Iteration: 9740    Accuracy: 1.0\n",
      "Iteration: 9750    Loss: 0.0023359698243439198\n",
      "Iteration: 9750    Accuracy: 1.0\n",
      "Iteration: 9760    Loss: 0.0023169824853539467\n",
      "Iteration: 9760    Accuracy: 1.0\n",
      "Iteration: 9770    Loss: 0.0023025835398584604\n",
      "Iteration: 9770    Accuracy: 1.0\n",
      "Iteration: 9780    Loss: 0.002295962069183588\n",
      "Iteration: 9780    Accuracy: 1.0\n",
      "Iteration: 9790    Loss: 0.002283090027049184\n",
      "Iteration: 9790    Accuracy: 1.0\n",
      "Iteration: 9800    Loss: 0.002274606842547655\n",
      "Iteration: 9800    Accuracy: 1.0\n",
      "Iteration: 9810    Loss: 0.0022610966116189957\n",
      "Iteration: 9810    Accuracy: 1.0\n",
      "Iteration: 9820    Loss: 0.002248242264613509\n",
      "Iteration: 9820    Accuracy: 1.0\n",
      "Iteration: 9830    Loss: 0.0022395660635083914\n",
      "Iteration: 9830    Accuracy: 1.0\n",
      "Iteration: 9840    Loss: 0.002230669604614377\n",
      "Iteration: 9840    Accuracy: 1.0\n",
      "Iteration: 9850    Loss: 0.0022207023575901985\n",
      "Iteration: 9850    Accuracy: 1.0\n",
      "Iteration: 9860    Loss: 0.0022106270771473646\n",
      "Iteration: 9860    Accuracy: 1.0\n",
      "Iteration: 9870    Loss: 0.0022021811455488205\n",
      "Iteration: 9870    Accuracy: 1.0\n",
      "Iteration: 9880    Loss: 0.002192168030887842\n",
      "Iteration: 9880    Accuracy: 1.0\n",
      "Iteration: 9890    Loss: 0.0021832275670021772\n",
      "Iteration: 9890    Accuracy: 1.0\n",
      "Iteration: 9900    Loss: 0.002173468703404069\n",
      "Iteration: 9900    Accuracy: 1.0\n",
      "Iteration: 9910    Loss: 0.0021644351072609425\n",
      "Iteration: 9910    Accuracy: 1.0\n",
      "Iteration: 9920    Loss: 0.0021556394640356302\n",
      "Iteration: 9920    Accuracy: 1.0\n",
      "Iteration: 9930    Loss: 0.0021466552279889584\n",
      "Iteration: 9930    Accuracy: 1.0\n",
      "Iteration: 9940    Loss: 0.0021377282682806253\n",
      "Iteration: 9940    Accuracy: 1.0\n",
      "Iteration: 9950    Loss: 0.0021288329735398293\n",
      "Iteration: 9950    Accuracy: 1.0\n",
      "Iteration: 9960    Loss: 0.0021198722533881664\n",
      "Iteration: 9960    Accuracy: 1.0\n",
      "Iteration: 9970    Loss: 0.0021110959351062775\n",
      "Iteration: 9970    Accuracy: 1.0\n",
      "Iteration: 9980    Loss: 0.002102224389091134\n",
      "Iteration: 9980    Accuracy: 1.0\n",
      "Iteration: 9990    Loss: 0.0020936166401952505\n",
      "Iteration: 9990    Accuracy: 1.0\n",
      "Iteration: 10000    Loss: 0.0020848701242357492\n",
      "Iteration: 10000    Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "\n",
    "batch_size = 64\n",
    "num_points = 64\n",
    "num_labels = 1\n",
    "\n",
    "pointnet = PointNet(num_points, num_labels)\n",
    "\n",
    "new_param = pointnet.state_dict()\n",
    "new_param['main.0.main.6.bias'] = torch.eye(3, 3).view(-1)\n",
    "new_param['main.3.main.6.bias'] = torch.eye(64, 64).view(-1)\n",
    "pointnet.load_state_dict(new_param)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(pointnet.parameters(), lr=0.001)\n",
    "\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for iteration in range(10000+1):\n",
    "\n",
    "    pointnet.zero_grad()\n",
    "    input, labels = data_sampler(batch_size, num_points)\n",
    "\n",
    "    output = pointnet(input)\n",
    "    output = nn.Sigmoid()(output)\n",
    "\n",
    "    error = criterion(output, labels)\n",
    "    error.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output[output > 0.5] = 1\n",
    "        output[output < 0.5] = 0\n",
    "        accuracy = (output==labels).sum().item()/batch_size\n",
    "\n",
    "    loss_list.append(error.item())\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    if iteration % 10 == 0:\n",
    "        print(f'Iteration: {iteration}    Loss: {error.item()}')\n",
    "        print(f'Iteration: {iteration}    Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}